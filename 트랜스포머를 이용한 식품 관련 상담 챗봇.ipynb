{"cells":[{"cell_type":"markdown","metadata":{"id":"evFmz8XyYlfQ"},"source":["# 데이터 로드 및 전처리"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6715,"status":"ok","timestamp":1699945215055,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"S4vhygPKYZur"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","import time\n","import tensorflow_datasets as tfds\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64481,"status":"ok","timestamp":1699945396839,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"PK6LWzOsfUJH","outputId":"ca393127-7227-403d-a5df-1186eec4ded6"},"outputs":[{"name":"stdout","output_type":"stream","text":["챗봇 샘플의 개수 : 217156\n","발화문 (Q)    0\n","발화문 (A)    0\n","QA번호       0\n","인텐트 (대)    0\n","인텐트 (중)    0\n","인텐트 (소)    0\n","dtype: int64\n"]}],"source":["train_data = pd.read_excel('/content/Transformer 식품 데이터.xlsx')\n","train_data.head()\n","\n","print('챗봇 샘플의 개수 :', len(train_data))\n","print(train_data.isnull().sum()) #Null 값 확인"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":856,"status":"ok","timestamp":1699945397692,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"OD8CRiIQhCcg"},"outputs":[],"source":["questions = []\n","for sentence in train_data['발화문 (Q)']:\n","    # 구두점에 대해서 띄어쓰기\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions.append(sentence)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1471,"status":"ok","timestamp":1699945399160,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"MrJeLCemhROe"},"outputs":[],"source":["answers = []\n","for sentence in train_data['발화문 (A)']:\n","    # 구두점에 대해서 띄어쓰기\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers.append(sentence)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699945399160,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"TM6i14h1haqO","outputId":"c7786f6a-9082-48ad-993b-daac627c6ba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["['컵밥 2만원 이상을 사면 무료 배송으로 아는데 여기가 죽도 추가로 몇 개 더 주문해도 추가 배송비 없나요 ?', '해마로 기준 6개 이상이고 볶음밥 기준으로는 50개 이상인데 해마로 케이준 1봉과 볶음밥 10봉 정도 같이 구매해도 각각 배송비 내야 하나요 ?', '마트 원데이는 무료 배송이라고 되있는데 왜 결제를 하려고 하니까 배송비 3000원이 붙는 건가요 ?', '제주도 추가 배송료 잇나요 ?', '물건이 오지도 않았는데 취소하게 되면 3천원이 빠지더라구 요 반품 택배사원이 오는 것도 아닌데 이 금액이 빠지는 이유는 어떻게 될까요 ?']\n","['박스에 합포가 되지 않을 수도 있어 정확한 답변 드리지 못하는 점 양해 부탁드립니다 .', '일단 두 제품이 같이 담기면 최소 수량 부과인 6개 이상은 시스템 상 자동부과가 됩니다 .', '1KG 5 팩 이 상 무료배송 상품입니다 .', '문의해주셔서 감사해요 .', '운송장 발행되어 택배사에서 발송하면 반품 배송비가 발생해요 .']\n"]}],"source":["print(questions[:5])\n","print(answers[:5])"]},{"cell_type":"markdown","metadata":{"id":"vd2o7w3IheNz"},"source":["# 단어 집합 생성"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":361290,"status":"ok","timestamp":1699945760447,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"_gr69ZEUhhbZ"},"outputs":[],"source":["# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1699945760447,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"LSkvbIxHiSDG","outputId":"afd8a7e2-36fe-45bf-a334-70ab443a31a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["시작 토큰 번호 : [8130]\n","종료 토큰 번호 : [8131]\n","단어 집합의 크기 : 8132\n"]}],"source":["# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","\n","# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n","VOCAB_SIZE = tokenizer.vocab_size + 2\n","\n","print('시작 토큰 번호 :',START_TOKEN)\n","print('종료 토큰 번호 :',END_TOKEN)\n","print('단어 집합의 크기 :',VOCAB_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"7T3R7kFjjbX0"},"source":["# 정수 인코딩과 패딩"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1699945760447,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"avq_SWiejZRn","outputId":"19f1146e-c6a0-4b29-c9db-a8a881adf282"},"outputs":[{"name":"stdout","output_type":"stream","text":["임의의 질문 샘플을 정수 인코딩 : [16, 939, 887, 5653, 27, 545, 476, 1724, 56, 6090, 109, 882, 7925, 5756, 62, 310, 72, 1]\n"]}],"source":["# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n","print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1699945760447,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"rKEmLLJkjgne","outputId":"deb27404-a2bb-4084-d035-c2d3e31c4f35"},"outputs":[{"name":"stdout","output_type":"stream","text":["정수 인코딩 후의 문장 [16, 939, 887, 5653, 27, 545, 476, 1724, 56, 6090, 109, 882, 7925, 5756, 62, 310, 72, 1]\n","기존 문장: 한 번에 40개만 주문 가능하네요 10개 더 주문하려는 데 배송비 3천원 어떻게 해야 하나요 ?\n"]}],"source":["# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n","# 임의의 입력 문장을 sample_string에 저장\n","sample_string = questions[20]\n","\n","# encode() : 텍스트 시퀀스 --\u003e 정수 시퀀스\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n","\n","# decode() : 정수 시퀀스 --\u003e 텍스트 시퀀스\n","original_string = tokenizer.decode(tokenized_string)\n","print ('기존 문장: {}'.format(original_string))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1699945760448,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"fKxT5cdHjw42","outputId":"558f79ac-75e8-40b8-aca3-8b32780d7614"},"outputs":[{"name":"stdout","output_type":"stream","text":["16 ----\u003e 한 \n","939 ----\u003e 번에 \n","887 ----\u003e 40\n","5653 ----\u003e 개만 \n","27 ----\u003e 주문 \n","545 ----\u003e 가능하\n","476 ----\u003e 네요 \n","1724 ----\u003e 10개 \n","56 ----\u003e 더 \n","6090 ----\u003e 주문하려는 \n","109 ----\u003e 데 \n","882 ----\u003e 배송비 \n","7925 ----\u003e 3\n","5756 ----\u003e 천원 \n","62 ----\u003e 어떻게 \n","310 ----\u003e 해야 \n","72 ----\u003e 하나요\n","1 ----\u003e  ?\n"]}],"source":["# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n","# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n","for ts in tokenized_string:\n","  print ('{} ----\u003e {}'.format(ts, tokenizer.decode([ts])))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1699945760448,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"Bfw7rAmaj1kS"},"outputs":[],"source":["# 최대 길이를 40으로 정의\n","MAX_LENGTH = 40\n","\n","# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n","def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24682,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"A4C-7vh0kAUf","outputId":"621df445-85f5-4adb-caba-cc5bffae08f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["질문 데이터의 크기(shape) : (217156, 40)\n","답변 데이터의 크기(shape) : (217156, 40)\n"]}],"source":["questions, answers = tokenize_and_filter(questions, answers)\n","\n","print('질문 데이터의 크기(shape) :', questions.shape)\n","print('답변 데이터의 크기(shape) :', answers.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"FYkhCOX7kHyl","outputId":"c4ab5515-c8a6-41ca-ea99-c334f8dc4e23"},"outputs":[{"name":"stdout","output_type":"stream","text":["[8130 1206 1292 7924 2093 1123   14 1453  932 1105  145  117 2023   15\n"," 1585   18 1076   77  236   56 1865  197  882  290    1 8131    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0]\n","[8130  690  949 6001 7906  360 2359  471  243  248  560 1004 3496   37\n","   51   39    2 8131    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0]\n"]}],"source":["# 0번 샘플을 임의로 출력\n","print(questions[0])\n","print(answers[0])"]},{"cell_type":"markdown","metadata":{"id":"XJnYl_dMkJiz"},"source":["# 인코더와 디코더의 입력, 그리고 레이블 만들기.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"11n9uNX8kLG6"},"outputs":[],"source":["# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n","# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': questions,\n","        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n","    },\n","    {\n","        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n","    },\n","))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"6O6LqJzykUl0","outputId":"d72b8b44-69d8-41d8-9a2a-cd2a7dc2337d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[8130  690  949 6001 7906  360 2359  471  243  248  560 1004 3496   37\n","   51   39    2 8131    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0]\n","[[8130  690  949 6001 7906  360 2359  471  243  248  560 1004 3496   37\n","    51   39    2 8131    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0]]\n","[[ 690  949 6001 7906  360 2359  471  243  248  560 1004 3496   37   51\n","    39    2 8131    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0]]\n"]}],"source":["# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n","print(answers[0]) # 기존 샘플\n","print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n","print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."]},{"cell_type":"markdown","metadata":{"id":"_7wHHwtRkbPD"},"source":["# 트랜스포머 만들기"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"F6534luH9d0K"},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value, mask):\n","  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n","\n","  # Q와 K의 곱. 어텐션 스코어 행렬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 스케일링\n","  # dk의 루트값으로 나눠준다.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n","  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n","  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"RNU5ilcw9f3q"},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_model을 num_heads로 나눈 값.\n","    # 논문 기준 : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WV에 해당하는 밀집층 정의\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    # WO에 해당하는 밀집층 정의\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  # num_heads 개수만큼 q, k, v를 split하는 함수\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n","    # q : (batch_size, query의 문장 길이, d_model)\n","    # k : (batch_size, key의 문장 길이, d_model)\n","    # v : (batch_size, value의 문장 길이, d_model)\n","    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. 헤드 나누기\n","    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n","    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. 헤드 연결(concatenate)하기\n","    # (batch_size, query의 문장 길이, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WO에 해당하는 밀집층 지나기\n","    # (batch_size, query의 문장 길이, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"ehmpN7bNmjTh"},"outputs":[],"source":["def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, key의 문장 길이)\n","  return mask[:, tf.newaxis, tf.newaxis, :]"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"nskamzrs9j1K"},"outputs":[],"source":["def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # 패딩 마스크 사용\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"34a0WK-B9lgi"},"outputs":[],"source":["def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785110,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"Ng1ysGoJ9mha"},"outputs":[],"source":["# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n","  return tf.maximum(look_ahead_mask, padding_mask)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785111,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"I3QDoLz49oPK"},"outputs":[],"source":["def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': look_ahead_mask # 룩어헤드 마스크\n","      })\n","\n","  # 잔차 연결과 층 정규화\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n","          'mask': padding_mask # 패딩 마스크\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785111,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"5i9lZmji9qDa"},"outputs":[],"source":["def decoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 디코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785111,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"noEmFlbklu6L"},"outputs":[],"source":["def transformer(vocab_size, num_layers, dff,\n","                d_model, num_heads, dropout,\n","                name=\"transformer\"):\n","\n","  # 인코더의 입력\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 디코더의 입력\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # 인코더의 패딩 마스크\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask, output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # 디코더의 패딩 마스크(두번째 서브층)\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n","  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n","\n","  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n","  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # 다음 단어 예측을 위한 출력층\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945785111,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"JY6lwO-oWZJd"},"outputs":[],"source":["# 최종 버전\n","class PositionalEncoding(tf.keras.layers.Layer):\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","\n","    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","\n","    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    angle_rads = np.zeros(angle_rads.shape)\n","    angle_rads[:, 0::2] = sines\n","    angle_rads[:, 1::2] = cosines\n","    pos_encoding = tf.constant(angle_rads)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","\n","    print(pos_encoding.shape)\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3719,"status":"ok","timestamp":1699945788825,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"6h3zfbXikZ8j","outputId":"b536ea15-ce4a-40c9-a37b-b6d89304cbf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 8132, 256)\n","(1, 8132, 256)\n"]}],"source":["tf.keras.backend.clear_session()\n","\n","# 하이퍼파라미터\n","D_MODEL = 256\n","NUM_LAYERS = 2\n","NUM_HEADS = 8\n","DFF = 512\n","DROPOUT = 0.1\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)"]},{"cell_type":"markdown","metadata":{"id":"59LQe3CTn-ur"},"source":["## 학습률과 옵티마이저를 정의하고 모델을 컴파일"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699945788825,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"ldapwQ2PBiLT"},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, dtype=tf.float32) # 추가 코드\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","  # def get_config(self):\n","  #   config=super().get_config().copy()\n","  #   return config\n","\n","  def get_config(self):\n","    config= super().get_config().copy()\n","    config.update({\n","             'labels': self.labels_array,\n","             'num_labels': self.num_labels,\n","         })\n","    return config"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945788825,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"WYz8V5q2n1cp"},"outputs":[],"source":["def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699945788825,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"w3qpxTy7Ainq"},"outputs":[],"source":["MAX_LENGTH = 40\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699945788825,"user":{"displayName":"Younghoon Kim","userId":"02909712154393873022"},"user_tz":-540},"id":"gS_v6AHenfkQ"},"outputs":[],"source":["learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"MzdY3y5Pbudm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","3394/3394 [==============================] - 7802s 2s/step - loss: 2.3882 - accuracy: 0.1014\n","Epoch 2/7\n","3394/3394 [==============================] - 7776s 2s/step - loss: 1.8070 - accuracy: 0.1367\n","Epoch 3/7\n","3394/3394 [==============================] - 7780s 2s/step - loss: 1.6756 - accuracy: 0.1481\n","Epoch 4/7\n","3394/3394 [==============================] - 7807s 2s/step - loss: 1.6076 - accuracy: 0.1549\n","Epoch 5/7\n","3394/3394 [==============================] - 7489s 2s/step - loss: 1.5645 - accuracy: 0.1594\n","Epoch 6/7\n","1192/3394 [=========\u003e....................] - ETA: 1:20:27 - loss: 1.4257 - accuracy: 0.1614"]}],"source":["# 총 50회의 모델을 학습\n","EPOCHS = 7\n","model.fit(dataset, epochs=EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75D4RovNGs6R"},"outputs":[],"source":["# 모델 불러오기\n","#loaded_model = load_model('my_model.h5')\n","\n","# 모델 다시 저장\n","#loaded_model.save('my_model_python3.8.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeND6pdHOdTW"},"outputs":[],"source":["# 모델 불러오기\n","#loaded_model = tf.keras.models.load_model(\"my_model\", custom_objects={'CustomSchedule': CustomSchedule})\n","\n","# CustomSchedule 클래스를 설정과 함께 다시 인스턴스화\n","#config = loaded_model.optimizer.learning_rate.get_config()\n","#custom_schedule = CustomSchedule(**config)\n","\n","# 모델 빌드\n","#loaded_model.optimizer.learning_rate = custom_schedule"]},{"cell_type":"markdown","metadata":{"id":"youVJv-rb5ui"},"source":["# 챗봇 평가하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYo3AGCwb7nJ"},"outputs":[],"source":["def preprocess_sentence(sentence):\n","  # 단어와 구두점 사이에 공백 추가.\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = sentence.strip()\n","  return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpYQ97xccAhU"},"outputs":[],"source":["def evaluate(sentence):\n","  # 입력 문장에 대한 전처리\n","  sentence = preprocess_sentence(sentence)\n","\n","  # 입력 문장에 시작 토큰과 종료 토큰을 추가\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  # 디코더의 예측 시작\n","  for i in range(MAX_LENGTH):\n","    predictions = model(inputs=[sentence, output], training=False)\n","\n","    # 현재 시점의 예측 단어를 받아온다.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n","    # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  # 단어 예측이 모두 끝났다면 output을 리턴.\n","  return tf.squeeze(output, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lq0GuJxcD_A"},"outputs":[],"source":["def predict(sentence):\n","  prediction = evaluate(sentence)\n","\n","  # prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\n","  # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i \u003c tokenizer.vocab_size])\n","\n","  print('Input: {}'.format(sentence))\n","  print('Output: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIS8aNp6Jmko"},"outputs":[],"source":["output = predict(\"배송 언제 되나요?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcMELc04KpWe"},"outputs":[],"source":["output = predict(\"오늘 배송이라고 뜨는데 아직 안왔습니다. 언제 도착하나요?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcOJpuTkK0Bc"},"outputs":[],"source":["output = predict(\"배송 날짜가 언제인가요?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lYTzv7dMw-o"},"outputs":[],"source":["output = predict(\"제품 추가 구매에 대한 추가 배송비가 있나요?\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNHQ2W1Bwjl7It98N5Z7kU9","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}